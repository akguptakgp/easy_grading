import warnings
import cPickle
from feature_extractor import *
import sys

with warnings.catch_warnings():
    warnings.filterwarnings("ignore",category=DeprecationWarning)
    import numpy as np
    import sklearn
    from sklearn.cross_validation import KFold
    from sklearn.svm import SVR, SVC
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.kernel_ridge import KernelRidge
    from sklearn.tree import DecisionTreeClassifier
    import csv
    import weighted_kappa as own_wp
    from sklearn.lda import LDA

class meta_non_linear(object):
    def __init__(self, learner):
        self.L = learner

    def __str__(self):
        return self.L.__str__()

    def train(self,X_train,Y_train):
        temp = []
        for i in range(len(Y_train)):
            temp.append(Y_train.item(i))
        self.L.fit(X_train,temp)

    def predict(self,X_test):
        d = self.L.predict(X_test)
        return d
## Without LDA,
## params are as follows -
## gamma=0.00003
## C=0.8
## Remember this
gamma = 0.003
svm_gamma = 0.025
C = 1
class support_vector_regression(meta_non_linear):
    def __init__(self):
        super(self.__class__, self).__init__(SVR(kernel='rbf', gamma=gamma))

class support_vector_machine(meta_non_linear):
    def __init__(self):
        super(self.__class__, self).__init__(SVC(kernel='rbf', gamma=svm_gamma, C=C))

class decision_tree_classifier(meta_non_linear):
    def __init__(self):
        super(self.__class__, self).__init__(DecisionTreeClassifier(criterion='entropy'))

class meta_linear(object):
    def __init__(self, learner):
        self.L = learner

    def __str__(self):
        return self.L.__str__()

    def train(self,X_train,Y_train):
        self.L.fit(X_train,Y_train)

    def predict(self,X_test):
        return self.L.predict(X_test)

class linear_regression(meta_linear):
    def __init__(self):
        super(self.__class__, self).__init__(LinearRegression(fit_intercept=True, normalize=True, copy_X=True))

class logistic_regression(meta_linear):
    def __init__(self):
        super(self.__class__, self).__init__(LogisticRegression(penalty='l2', dual=False, C=0.8, fit_intercept=True, solver='lbfgs', multi_class='multinomial'))

class k_fold_cross_validation(object):
    '''
        The class will take an statistical class and training set and parameter k.
        The set will be divided wrt to k and cross validated using the statistical
        class provided.
        The statistical class should have two methods and no constructor args -
        method train(training_x, training_y)
        method predict(x_test_val)
    '''
    def __init__(self,k,stat_class,x_train,y_train,range_min,range_max):
        self.k_cross = float(k)
        self.stat_class = stat_class
        self.x_train = x_train
        self.y_train = y_train
        self.values = []
        self.range_min = range_min
        self.range_max = range_max

    def execute(self,i,j):
        # dim_red = LDA()
        # dim_red.fit_transform(self.x_train, self.y_train)
        # with open('dumped_dim_red_'+str(i)+'.pkl', 'wb') as fid:
        #     cPickle.dump(dim_red, fid)

        # x_train = dim_red.transform(self.x_train)
        # x_test = dim_red.transform(self.y_train)    
        # stat_obj = self.stat_class() # reflection bitches
        # stat_obj.train(x_train, x_test)
        # print len(x_train)
        # with open('dumped_'+str(j)+'_'+str(i)+'.pkl', 'wb') as fid:
        #     cPickle.dump(stat_obj, fid)

        kf = KFold(len(self.x_train), n_folds=self.k_cross)
        own_kappa = []
        for train_idx, test_idx in kf:
		# print train_idx,test_idx
		# exit(0)
            x_train, x_test = self.x_train[train_idx], self.x_train[test_idx]
            y_train, y_test = self.y_train[train_idx], self.y_train[test_idx]
            dim_red = LDA()
            x_train = dim_red.fit_transform(x_train, y_train)
			
			
            # with open('dumped_dim_red_'+str(i)+'.pkl', 'wb') as fid:
            #     cPickle.dump(dim_red, fid)

            # with open('dumped_dim_red_'+str(i)+'.pkl', 'rb') as fid:
                # dim_red=cPickle.load(fid)
            x_test = dim_red.transform(x_test)
                
            # with open('dumped_'+str(j)+'_'+str(i)+'.pkl', 'rb') as fid:
            #     stat_obj=cPickle.load(fid)
            # x_train = dim_red.transform(x_train)
            # x_test = dim_red.transform(x_test)

            stat_obj = self.stat_class() # reflection bitches
            stat_obj.train(x_train,y_train)
            # with open('dumped_'+str(j)+'_'+str(i)+'.pkl', 'wb') as fid:
                # cPickle.dump(stat_obj, fid)
            # with open('dumped_'+str(j)+'_'+str(i)+'.pkl', 'rb') as fid:
                # stat_obj=cPickle.load(fid)
            y_pred = [ 0 for i in xrange(len(y_test)) ]
            for i in range(len(x_test)):
                # print len(x_test[i])
                val = int(np.round(stat_obj.predict(x_test[i])))
                if val > self.range_max: val = self.range_max
                if val < self.range_min: val = self.range_min
                y_pred[i] = [val]
            y_pred = np.matrix(y_pred)
            cohen_kappa_rating = own_wp.quadratic_weighted_kappa(y_test,y_pred,self.range_min,self.range_max)
            self.values.append(cohen_kappa_rating)
        return str(sum(self.values)/self.k_cross)


def data_manipulation():
    linear_accuracy = []
    logistic_accuracy = []
    svr_accuracy = []
    svm_accuracy = []
    kernel_regress_accuracy = []
    decision_tree_accuracy = []
    for i in [1,2,3,4,5,6,7,8]: #to change after feature extraction done for all sets
        # training data
        train_data = []
        # print './Data/features_'+str(i)+'.csv'
        with open('../features/features_'+str(i)+'.csv','r') as in_file:
             csv_content = list(csv.reader(in_file,delimiter=','))
             for row in csv_content:
                train_data.append(row)
        header = train_data[0]
        train_data = train_data[1:]   #clip the header
        train_data = np.matrix(train_data,dtype='float64')
        Y_train = train_data[:,2].copy()     #actual_values
        X_train = train_data[:,2:].copy()    #actual_data with random bias units
        m = np.size(X_train,axis=0)
        X_train[:,0] = np.ones((m,1)) #bias units modified
        # print X_train[0]
        # print Y_train[0]
        # continue
        cross_valid_k = 5
        range_max = range_min = 0
        if i == 1:
            range_min = 2
            range_max = 12
        elif i==2:
            range_min = 1
            range_max = 6       
        elif i == 3 or i == 4:
            range_min=0
            range_max = 3
        elif i == 5 or i == 6:
            range_min=0
            range_max = 4
        elif i==7:
            range_min=0
            range_max = 30
        elif i==8:
            range_min=0
            range_max = 60    
        linear_k_cross = k_fold_cross_validation(cross_valid_k,linear_regression,X_train,Y_train,range_min,range_max)
        linear_accuracy.append(linear_k_cross.execute(i,0))
        logistic_k_cross = k_fold_cross_validation(cross_valid_k,logistic_regression,X_train,Y_train,range_min,range_max)
        logistic_accuracy.append(logistic_k_cross.execute(i,1))
        svr_k_cross = k_fold_cross_validation(cross_valid_k,support_vector_regression,X_train,Y_train,range_min,range_max)
        svr_accuracy.append(svr_k_cross.execute(i,2))
        svm_k_cross = k_fold_cross_validation(cross_valid_k,support_vector_machine,X_train,Y_train, range_min,range_max)
        svm_accuracy.append(svm_k_cross.execute(i,3))
    print " linear_regression :\n" + str(linear_accuracy)
    print " logistic_regression :\n" + str(logistic_accuracy)
    print " support_vector_regression :\n" + str(svr_accuracy)
    print " support_vector_machine :\n" + str(svm_accuracy)

def Bag_of_Words1(essay_tokens, all_words,i):
    count = Counter(all_words)
    common_words = [x[0] for x in count.most_common(10)]
    essays  = [" ".join([str(word) for word in e if word in common_words]) for e in essay_tokens]
    # tfidf = TfidfVectorizer(tokenizer=stem_tokenize, stop_words='english')
    with open('dumped_tfidf_'+str(i)+'.pkl', 'rb') as fid:
        tfidf=cPickle.load(fid)
    # print essays
    tfs = tfidf.transform(essays)
    feature_names = tfidf.get_feature_names()
    v = []
    for i in xrange(len(essay_tokens)):
       value = 0
       count_essay = Counter(essay_tokens[i])
       common_w_essay = count_essay.most_common(10)
       common_words_essay = [word[0] for word in common_w_essay]
       for j in xrange(len(common_words_essay)):
           if common_words_essay[j] in common_words:
               for col in tfs.nonzero()[1]:
                  if feature_names[col] == common_words_essay[j]:
                           value += common_w_essay[j][1]*tfs[0,col]
                           break
       v.append(value)
    return v
def generate_feature(f1,i):
    with open(f1,'rb') as f:
        all_words = []
        essays = []
        csv_rows = list(csv.reader(f, delimiter = ','))
        essay_tokens = []
        points = []
        print len(csv_rows)
        for row in csv_rows:
            p = Point(row[0], row[1], row[2], int(row[6]))
            tokens = stem_tokenize(row[2].translate(None, string.punctuation).decode('utf-8'))
            essay_tokens.append(tokens)  
            all_words.extend(tokens)
            points.append(p)
        values = Bag_of_Words1(essay_tokens, all_words,i)
        for i in xrange(len(points)):
            points[i].set_bag_of_words(values[i])
        out_file = open('feature.csv','w')
        out_file.write(points[0].get_label())
        for p in points:
            out_file.write(str(p))

def predict(f,i,j):
    print "generate_feature"
    generate_feature(f,i)
    print "generate_feature_done"
    # with open('dumped_dim_red_'+str(i)+'.pkl', 'rb') as fid:
    #     dim_red=cPickle.load(fid)
    # with open('dumped_'+str(j)+'_'+str(i)+'.pkl', 'rb') as fid:
    #     stat_obj=cPickle.load(fid)
    # train_data = []
    # with open('feature.csv','r') as in_file:
    #     csv_content = list(csv.reader(in_file,delimiter=','))
    #     for row in csv_content:
    #         train_data.append(row)
    # #         # print
    # header = train_data[0]
    # train_data = train_data[1:]   #clip the header
    # train_data = np.matrix(train_data,dtype='float64')
    # print train_data
    # Y_train = train_data[:,2].copy()     #actual_values
    # X_train = train_data[:,2:-1].copy()    #actual_data with random bias units
    # m = np.size(X_train,axis=0)
    # X_train[:,0] = np.ones((m,1)) #bias units modified
    # cross_valid_k = 5
    # range_max = range_min = 0
    # if i == 1:
    #     range_min = 2
    #     range_max = 12
    # elif i==2:
    #     range_min = 1
    #     range_max = 6       
    # elif i == 3 or i == 4:
    #     range_min=0
    #     range_max = 3
    # elif i == 5 or i == 6:
    #     range_min=0
    #     range_max = 4
    # elif i==7:
    #     range_min=0
    #     range_max = 30
    # elif i==8:
    #     range_min=0
    #     range_max = 60    
    # # x_test = dim_red.transform(X_train)
    # val = int(np.round(stat_obj.predict(X_train)))
    # print " experiment "+str(j)+" :"
    # print " linear_regression :\t" +  str(np.average(linear_accuracy)) + '\t'+str(linear_accuracy)
    # print " logistic_regression :\t" +str(np.average(logistic_accuracy)) + '\t'+ str(logistic_accuracy)
    # print " support_vector_regression :\t" +str(np.average(svr_accuracy)) + '\t'+ str(svr_accuracy)
    # print " support_vector_machine :\t" + str(np.average(svm_accuracy)) + '\t'+str(svm_accuracy)
    # print " decision_tree_classifier :\t" + str(np.average(decision_tree_accuracy)) + '\t'+str(decision_tree_accuracy)    

if __name__=='__main__':
    data_manipulation()
    predict   ('input.txt',1,0)
